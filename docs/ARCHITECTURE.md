# AndroidVisionAutomator: Autonomous Android MCP

**Goal (one sentence):** Build an AI-driven Android agent that converts voice/text â†’ intent â†’ UI actions, using Accessibility Services as hands and Claude LLM as the brain.

---

## ğŸ§± System Architecture

```
User (Voice/Text)
      â†“
[1] Input Layer (SpeechRecognizer / Text Input)
      â†“
[2] Intent Parser (Claude LLM)
      â†“
[3] Task Planner (Claude LLM) 
      â†“
[4] Action Executor (Accessibility Service)
      â†“
[5] Screen Feedback (UI Tree / OCR)
      â†“
[6] Verification & Replanning Loop
      â†“
[7] Safety & Permissions Layer
```

---

## ğŸ“ Project Structure

```
AndroidVisionAutomator/
â”œâ”€â”€ android-app/                          # Android application
â”‚   â”œâ”€â”€ build.gradle.kts                  # Gradle build config
â”‚   â”œâ”€â”€ app/src/main/
â”‚   â”‚   â”œâ”€â”€ AndroidManifest.xml           # Permissions & services
â”‚   â”‚   â””â”€â”€ java/com/autonomousvision/
â”‚   â”‚       â”œâ”€â”€ models/
â”‚   â”‚       â”‚   â”œâ”€â”€ Intent.kt             # Data models
â”‚   â”‚       â”‚   â””â”€â”€ SafetyPolicy.kt       # Safety rules
â”‚   â”‚       â”œâ”€â”€ accessibility/
â”‚   â”‚       â”‚   â”œâ”€â”€ AutomationAccessibilityService.kt  # Core "muscles"
â”‚   â”‚       â”‚   â””â”€â”€ ScreenAnalyzer.kt     # Screen capture & parsing
â”‚   â”‚       â”œâ”€â”€ agent/
â”‚   â”‚       â”‚   â””â”€â”€ AgentExecutorService.kt  # Orchestrator
â”‚   â”‚       â””â”€â”€ safety/
â”‚   â”‚           â””â”€â”€ SafetyManager.kt      # Permission checker
â”‚   â”‚
â”œâ”€â”€ backend/                              # Python backend (cloud)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ claude_llm_client.py          # Claude API integration
â”‚   â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ mcp-server/                           # MCP server components
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ android_vision_mcp.py         # Main orchestrator
â”‚
â”œâ”€â”€ config.json                           # Global configuration
â”œâ”€â”€ examples.py                           # Usage examples
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ARCHITECTURE.md                   # This file
â””â”€â”€ README.md                             # Quick start guide
```

---

## ğŸ”„ The 7 Layers Explained

### 1ï¸âƒ£ Input Layer (Human â†’ Text)

**Files:** `AutomationAccessibilityService.kt`

Converts human voice or text to a command string.

```kotlin
// Voice input
val recognizer = SpeechRecognizer.createSpeechRecognizer(context)
recognizer.startListening(intent)

// Text input
val userCommand = "Send a message to Mom"
```

**Output:** Normalized command string â†’ Layer 2

---

### 2ï¸âƒ£ Intent Understanding (LLM)

**Files:** `claude_llm_client.py`, `IntentParser`

Parses natural language into structured intent using Claude.

```python
# Input: "Send a message to Mom saying I'll be late"
# Output: 
{
  "intent": "send_message",
  "target_app": "WhatsApp",
  "entities": {
    "contact": "Mom",
    "message": "I'll be late"
  }
}
```

**Responsibilities:**
- Understand what user wants
- Extract intent type
- Identify target app
- Extract entities (names, numbers, text)
- App-agnostic (not tied to specific UI)

---

### 3ï¸âƒ£ Task Planner (LLM)

**Files:** `TaskPlanner`, `claude_llm_client.py`

Converts intent + current screen state â†’ sequence of concrete UI actions.

```python
# Input:
{
  "intent": "send_message",
  "target_app": "WhatsApp",
  "entities": {"contact": "Mom", "message": "I'll be late"},
  "current_screen": {
    "app": "com.android.launcher",
    "visible_texts": ["WhatsApp", "Messages", "Settings"]
  }
}

# Output:
[
  {"action": "open_app", "value": "com.whatsapp"},
  {"action": "find_text", "target": "Mom"},
  {"action": "click", "target": "Mom"},
  {"action": "setText", "value": "I'll be late"},
  {"action": "click", "target": "send"}
]
```

**Key Principle:** Planner does NOT click directly â€” it issues abstract commands. Execution layer finds specific UI elements.

---

### 4ï¸âƒ£ Action Executor (Accessibility Service)

**Files:** `AutomationAccessibilityService.kt`

The "hands" â€” executes abstract actions on real Android UI.

```kotlin
// Supported actions:
- click(target: String)           // Find & click
- setText(value: String)          // Type text
- scroll(direction: String)       // up/down
- open_app(package: String)       // Launch app
- find_text(target: String)       // Verify exists
- back()                          // Android back
- home()                          // Home screen
- wait(duration: Long)            // Pause

// Each action reports back:
ActionResult {
  status: "SUCCESS" | "FAILED" | "ELEMENT_NOT_FOUND",
  errorMessage: "...",
  screenStateAfter: ScreenState
}
```

**Element Matching Strategy:**
1. Find node by text (case-insensitive)
2. Find by contentDescription
3. Find by className
4. If multiple matches, use index

---

### 5ï¸âƒ£ Screen Understanding (Feedback Loop)

**Files:** `ScreenAnalyzer.kt`

Captures current UI state and converts to LLM-friendly format.

```kotlin
ScreenState {
  currentApp: "com.whatsapp",
  visibleTexts: ["Mom", "Type a message", "Send"],
  focusedElement: "message_input",
  uiTree: "XML tree of accessibility nodes",
  screenshotBase64: "optional for OCR"
}
```

**Used for:**
- Verifying action success
- Replanning when element not found
- Context for next action decision

---

### 6ï¸âƒ£ Verification & Replanning

**Files:** `VerificationLoop`, `AgentExecutorService.kt`

The autonomy layer â€” detects failures and adapts.

```
For each action:
  1. Execute action
  2. Wait for screen update
  3. Verify expected state appeared
  
  If verification fails:
    - Extract why it failed
    - Send replan request to LLM
    - Get alternative action sequence
    - Resume from point of failure
    
  If after 3 retries still fails:
    - Mark task as failed
    - Report to user
```

**Example Failure Scenario:**
```
Original action: click("Mom")
Expected: Mom's chat opens
Actual: Chat list shown (Mom not visible)

Replan: scroll_down â†’ find_text("Mom") â†’ click("Mom")
```

---

### 7ï¸âƒ£ Safety & Permissions Layer

**Files:** `SafetyManager.kt`, `SafetyPolicy.kt`

Prevents harm, maintains Play Store compliance.

```kotlin
Safety Rules:
â”œâ”€â”€ Allowed Apps (whitelist)
â”‚   â”œâ”€â”€ WhatsApp
â”‚   â”œâ”€â”€ Google Maps
â”‚   â”œâ”€â”€ YouTube
â”‚   â””â”€â”€ [configurable]
â”‚
â”œâ”€â”€ Dangerous Actions (blocked)
â”‚   â”œâ”€â”€ delete_file
â”‚   â”œâ”€â”€ uninstall_app
â”‚   â”œâ”€â”€ change_settings
â”‚   â””â”€â”€ send_payment
â”‚
â”œâ”€â”€ Sensitive Actions (require confirmation)
â”‚   â”œâ”€â”€ send_message
â”‚   â”œâ”€â”€ make_call
â”‚   â””â”€â”€ send_email
â”‚
â””â”€â”€ Limits
    â”œâ”€â”€ max_actions_per_task: 50
    â”œâ”€â”€ max_retry_count: 3
    â””â”€â”€ task_timeout: 5 minutes
```

**Permission Check:**
```kotlin
enum ActionPermissionLevel {
  ALLOWED,                    // Execute immediately
  REQUIRES_CONFIRMATION,      // Ask user first
  DANGEROUS,                  // Explicit user approval
  BLOCKED                     // Never execute
}
```

---

## ğŸš€ Execution Flow Example

**User says:** "Send a message to Mom saying I'll be late"

```
[1] INPUT LAYER
    Voice â†’ "Send a message to Mom saying I'll be late"

[2] INTENT PARSER
    LLM extracts:
    {
      "intent": "send_message",
      "target_app": "WhatsApp",
      "entities": {"contact": "Mom", "message": "I'll be late"}
    }

[3] TASK PLANNER
    Current screen: Home screen with WhatsApp icon visible
    LLM creates plan:
    [
      {"action": "open_app", "value": "com.whatsapp"},
      {"action": "find_text", "target": "Mom"},
      {"action": "click", "target": "Mom"},
      {"action": "click", "target": "message input"},
      {"action": "setText", "value": "I'll be late"},
      {"action": "click", "target": "send"}
    ]

[4] ACTION EXECUTOR - Action 1
    execute: open_app(com.whatsapp)
    â†’ WhatsApp opens
    âœ… SUCCESS

[5] SCREEN FEEDBACK
    Current app: com.whatsapp
    Visible: ["Chats", "My Status", "Calls", "Settings"]
    â†’ No "Mom" visible

[6] VERIFICATION FAILS
    Expected: See "Mom" in chat list
    Actual: Mom not visible (probably need to scroll)
    
    Trigger replan:
    New plan: scroll_down â†’ find "Mom" â†’ click

[7] SAFETY CHECK
    All actions whitelisted?
    âœ… Yes (WhatsApp is allowed, send_message requires confirmation)
    User confirms via notification.

[4] ACTION EXECUTOR - Retry with new plan
    Continued execution with replanned actions...
    
[âœ…] TASK COMPLETE
    Message sent to Mom
```

---

## ğŸ” Safety Guarantees

1. **Whitelist Only:** Only configured apps can be automated
2. **Dangerous Action Blocking:** Payment, deletion, uninstall blocked
3. **Sensitive Action Confirmation:** Messages, calls require user approval
4. **Rate Limiting:** Max actions/task and max retries prevent loops
5. **Timeout Protection:** Tasks killed after 5 minutes
6. **Kill Switch:** User can stop any task instantly
7. **Audit Logging:** All actions logged for review

---

## ğŸ“‹ Configuration

See `config.json` for:
- Allowed apps whitelist
- Blocked/sensitive actions
- Max retries and timeouts
- Logging level
- LLM model and API settings

---

## ğŸ› ï¸ Development

### Android Setup
```bash
cd android-app
./gradlew build
./gradlew installDebug
```

### Backend Setup
```bash
pip install anthropic
export ANTHROPIC_API_KEY=sk-...
python examples.py
```

### Testing
```bash
# Unit tests
./gradlew test

# Integration tests
python -m pytest backend/tests/
```

---

## âš ï¸ Limitations & Future Work

**Current Limitations:**
- Single task at a time (no parallelization)
- Screen understanding relies on text (no image ML)
- No persistent learning (doesn't improve over time)
- Limited to UI automation (no system commands)

**Future Enhancements:**
- Vision model integration (Claude Vision)
- Multi-task execution
- Gesture recognition (long press, swipe)
- Custom action plugins
- Performance analytics
- Speech synthesis for feedback

---

## ğŸ“š References

- [Android Accessibility Service API](https://developer.android.com/reference/android/accessibilityservice/AccessibilityService)
- [Claude API Docs](https://docs.anthropic.com)
- [Model Context Protocol](https://modelcontextprotocol.io)

---

## ğŸ“„ License

MIT License - See LICENSE file

---

**Built with â¤ï¸ using Claude 3.5 Sonnet**
